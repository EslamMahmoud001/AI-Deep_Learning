{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EslamMahmoud001/AI-Deep_Learning/blob/main/Deep_Convolutional_Q_Learning_for_Pac_Man.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Deep Convolutional Q-Learning for Pac-Man"
      ],
      "metadata": {
        "id": "EAiHVEoWHy_D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 0 - Installing the required packages and importing the libraries"
      ],
      "metadata": {
        "id": "tjO1aK3Ddjs5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Installing Gymnasium"
      ],
      "metadata": {
        "id": "NwdRB-ZLdrAV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dbnq3XpoKa_7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "581998ac-ff0f-4557-9c40-7f6cfe200948"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gymnasium\n",
            "  Downloading gymnasium-1.0.0-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (4.12.2)\n",
            "Collecting farama-notifications>=0.0.1 (from gymnasium)\n",
            "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl.metadata (558 bytes)\n",
            "Downloading gymnasium-1.0.0-py3-none-any.whl (958 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m958.1/958.1 kB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Installing collected packages: farama-notifications, gymnasium\n",
            "Successfully installed farama-notifications-0.0.4 gymnasium-1.0.0\n",
            "Requirement already satisfied: gymnasium[accept-rom-license,atari] in /usr/local/lib/python3.10/dist-packages (1.0.0)\n",
            "\u001b[33mWARNING: gymnasium 1.0.0 does not provide the extra 'accept-rom-license'\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[accept-rom-license,atari]) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[accept-rom-license,atari]) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[accept-rom-license,atari]) (4.12.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium[accept-rom-license,atari]) (0.0.4)\n",
            "Collecting ale-py>=0.9 (from gymnasium[accept-rom-license,atari])\n",
            "  Downloading ale_py-0.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.6 kB)\n",
            "Downloading ale_py-0.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ale-py\n",
            "Successfully installed ale-py-0.10.1\n",
            "Requirement already satisfied: ale-py in /usr/local/lib/python3.10/dist-packages (0.10.1)\n",
            "Requirement already satisfied: numpy>1.20 in /usr/local/lib/python3.10/dist-packages (from ale-py) (1.26.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from ale-py) (4.12.2)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  swig4.0\n",
            "Suggested packages:\n",
            "  swig-doc swig-examples swig4.0-examples swig4.0-doc\n",
            "The following NEW packages will be installed:\n",
            "  swig swig4.0\n",
            "0 upgraded, 2 newly installed, 0 to remove and 49 not upgraded.\n",
            "Need to get 1,116 kB of archives.\n",
            "After this operation, 5,542 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 swig4.0 amd64 4.0.2-1ubuntu1 [1,110 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/universe amd64 swig all 4.0.2-1ubuntu1 [5,632 B]\n",
            "Fetched 1,116 kB in 1s (1,526 kB/s)\n",
            "Selecting previously unselected package swig4.0.\n",
            "(Reading database ... 123634 files and directories currently installed.)\n",
            "Preparing to unpack .../swig4.0_4.0.2-1ubuntu1_amd64.deb ...\n",
            "Unpacking swig4.0 (4.0.2-1ubuntu1) ...\n",
            "Selecting previously unselected package swig.\n",
            "Preparing to unpack .../swig_4.0.2-1ubuntu1_all.deb ...\n",
            "Unpacking swig (4.0.2-1ubuntu1) ...\n",
            "Setting up swig4.0 (4.0.2-1ubuntu1) ...\n",
            "Setting up swig (4.0.2-1ubuntu1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Requirement already satisfied: gymnasium[box2d] in /usr/local/lib/python3.10/dist-packages (1.0.0)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (4.12.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (0.0.4)\n",
            "Collecting box2d-py==2.3.5 (from gymnasium[box2d])\n",
            "  Downloading box2d-py-2.3.5.tar.gz (374 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.4/374.4 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pygame>=2.1.3 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (2.6.1)\n",
            "Collecting swig==4.* (from gymnasium[box2d])\n",
            "  Downloading swig-4.3.0-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl.metadata (3.5 kB)\n",
            "Downloading swig-4.3.0-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m64.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: box2d-py\n",
            "  Building wheel for box2d-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for box2d-py: filename=box2d_py-2.3.5-cp310-cp310-linux_x86_64.whl size=2349110 sha256=9115d189d3b498dd54c16e66138852da2daece7db4b3840563d74864e39f5ec9\n",
            "  Stored in directory: /root/.cache/pip/wheels/db/8f/6a/eaaadf056fba10a98d986f6dce954e6201ba3126926fc5ad9e\n",
            "Successfully built box2d-py\n",
            "Installing collected packages: swig, box2d-py\n",
            "Successfully installed box2d-py-2.3.5 swig-4.3.0\n"
          ]
        }
      ],
      "source": [
        "!pip install gymnasium\n",
        "!pip install \"gymnasium[atari, accept-rom-license]\"\n",
        "!pip install ale-py\n",
        "!apt-get install -y swig\n",
        "!pip install gymnasium[box2d]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Importing the libraries"
      ],
      "metadata": {
        "id": "H-wes4LZdxdd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ho_25-9_9qnu"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from collections import deque\n",
        "from torch.utils.data import DataLoader, TensorDataset"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 1 - Building the AI"
      ],
      "metadata": {
        "id": "m7wa0ft8e3M_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating the architecture of the Neural Network"
      ],
      "metadata": {
        "id": "dlYVpVdHe-i6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Network(nn.Module): #Inherit from nn module\n",
        "  def __init__(self, action_size, seed = 42):\n",
        "    super(Network, self).__init__()\n",
        "    self.seed = torch.manual_seed(seed)\n",
        "\n",
        "    # building the eyes (convolution layers network)\n",
        "\n",
        "    #Convolutional layer 1\n",
        "    self.conv1 = nn.Conv2d(3, 32, kernel_size=8, stride = 4) # 3 input channels (rgb), 32 output channels (convolutional layer) for pacman game, 8x8 kernel size, stride = 4\n",
        "    # Batch Normalization operation for layer 1\n",
        "    self.bn1 = nn.BatchNorm2d(32) # 32 is the number of output channells of convonutional layer 1\n",
        "\n",
        "    #Convolutional layer 2\n",
        "    self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride = 2) # 32 input channels (convolutional layer 1), 64 output channels (convolutional layer 2) for pacman game, 4x4 kernel size, stride = 2\n",
        "    # Batch Normalization operation for layer 2\n",
        "    self.bn2 = nn.BatchNorm2d(64) # 64 is the number of output channells of convonutional layer 2\n",
        "\n",
        "    #Convolutional layer 3\n",
        "    self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride = 1) # 64 input channels (convolutional layer 2), 64 output channels (convolutional layer 3) for pacman game, 3x3 kernel size, stride = 1\n",
        "    # Batch Normalization operation for layer 3\n",
        "    self.bn3 = nn.BatchNorm2d(64) # 64 is the number of output channells of convonutional layer 3\n",
        "\n",
        "    #Convolutional layer 4\n",
        "    self.conv4 = nn.Conv2d(64, 128, kernel_size=3, stride = 1) # 64 input channels (convolutional layer 3) , 128 output channels (convolutional layer 4) for pacman game, 3x3 kernel size, stride = 1\n",
        "    # Batch Normalization operation for layer 4\n",
        "    self.bn4 = nn.BatchNorm2d(128) # 128 is the number of convonutional layer 4\n",
        "\n",
        "    # Now agent have eyes\n",
        "\n",
        "    # building the brain (full connection layers neural network)\n",
        "\n",
        "    #flattening formula for each convolutional layer\n",
        "    # The Pacman game typically uses an input size of  210 × 160 × 3 210×160×3 (height  𝐻 = 210 H=210, width  𝑊 = 160 W=160, and 3 color channels for RGB). Assuming no padding is applied ( Padding = 0 Padding=0):\n",
        "    # Recalculate the output size of each convolutional layer based on the input size (128x128),\n",
        "    # kernel size, stride, and padding using the formulas:\n",
        "    #\n",
        "    # H_out = floor((H_in - kernel_size + 2 * padding) / stride) + 1\n",
        "    # W_out = floor((W_in - kernel_size + 2 * padding) / stride) + 1\n",
        "    #\n",
        "    # Convolutional Layers:\n",
        "    # 1. Conv1:\n",
        "    #    H_in = 128, W_in = 128, kernel_size = 8, stride = 4, padding = 0\n",
        "    #    H_out = floor((128 - 8 + 2 * 0) / 4) + 1 = 31\n",
        "    #    W_out = floor((128 - 8 + 2 * 0) / 4) + 1 = 31\n",
        "    #    Resulting size: 31x31, output channels = 32\n",
        "    #\n",
        "    # 2. Conv2:\n",
        "    #    H_in = 31, W_in = 31, kernel_size = 4, stride = 2, padding = 0\n",
        "    #    H_out = floor((31 - 4 + 2 * 0) / 2) + 1 = 14\n",
        "    #    W_out = floor((31 - 4 + 2 * 0) / 2) + 1 = 14\n",
        "    #    Resulting size: 14x14, output channels = 64\n",
        "    #\n",
        "    # 3. Conv3:\n",
        "    #    H_in = 14, W_in = 14, kernel_size = 3, stride = 1, padding = 0\n",
        "    #    H_out = floor((14 - 3 + 2 * 0) / 1) + 1 = 12\n",
        "    #    W_out = floor((14 - 3 + 2 * 0) / 1) + 1 = 12\n",
        "    #    Resulting size: 12x12, output channels = 64\n",
        "    #\n",
        "    # 4. Conv4:\n",
        "    #    H_in = 12, W_in = 12, kernel_size = 3, stride = 1, padding = 0\n",
        "    #    H_out = floor((12 - 3 + 2 * 0) / 1) + 1 = 10\n",
        "    #    W_out = floor((12 - 3 + 2 * 0) / 1) + 1 = 10\n",
        "    #    Resulting size: 10x10, output channels = 128\n",
        "    #\n",
        "    # Final Flattened Size:\n",
        "    # 10 * 10 * 128 = 12,800\n",
        "\n",
        "\n",
        "\n",
        "    self.fc1 = nn.Linear(10*10*128, 512) # 512 neurons for the first fully connected layer by experiment\n",
        "    self.fc2 = nn.Linear(512, 256)\n",
        "    self.fc3 = nn.Linear(256, action_size)\n",
        "\n",
        "    # Now the agent have brain\n",
        "\n",
        "\n",
        "    # Implementing the forward propagation\n",
        "\n",
        "  def forward(self, state): #input is the state because its gonna propagate the state from the input layer to the output layer\n",
        "\n",
        "    # *****propagating from the image to the convolutional layer\n",
        "\n",
        "    # Signal from Images to 1st conv layer then from from 1st conv to 1st batch normalization layer, then activate it using rectifier func\n",
        "    x = F.relu(self.bn1(self.conv1(state)))\n",
        "\n",
        "    # Signal from 1st conv layer to 2nd conv layer then from  2nd conv to 2nd batch normalization layer, then activate it using rectifier func\n",
        "    x = F.relu(self.bn2(self.conv2(x)))\n",
        "\n",
        "    # Signal from 2nd conv layer to 3rd conv layer then from  3rd conv to 3rd batch normalization layer, then activate it using rectifier func\n",
        "    x = F.relu(self.bn3(self.conv3(x)))\n",
        "\n",
        "    # Signal from 3rd conv layer to 4th conv layer then from  4th conv to 4th batch normalization layer, then activate it using rectifier func\n",
        "    x = F.relu(self.bn4(self.conv4(x)))\n",
        "\n",
        "    # reshape for flattening\n",
        "    x = x.view(x.size(0), -1)\n",
        "\n",
        "    # ******* propagating from the conv layers to ANN\n",
        "\n",
        "    # propagate the signal from the input layer to first fully connected layer with rectfier activation function\n",
        "    x = self.fc1(x) # take the state as the input to the first fully connected layer\n",
        "    x = F.relu(x) # assigning it to rectifier activation function\n",
        "\n",
        "    # propagate the signal from the  first fully connected layer to the second fully connected layer with rectfier activation function\n",
        "    x = self.fc2(x) # take the first fully connected layer output as the input to the second fully connected layer\n",
        "    x = F.relu(x) # assigning it to rectifier activation function\n",
        "\n",
        "    # propagate the signal from the  second fully connected layer to the output with rectfier activation function\n",
        "    return self.fc3(x) # take the second fully connected layer output as the input of the output layer\n"
      ],
      "metadata": {
        "id": "eUezIutUc0KD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 2 - Training the AI"
      ],
      "metadata": {
        "id": "rUvCfE_mhwo2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setting up the environment"
      ],
      "metadata": {
        "id": "WWCDPF22lkwc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import ale_py\n",
        "import gymnasium as gym\n",
        "env = gym.make('MsPacmanDeterministic-v0', full_action_space = False)\n",
        "state_shape = env.observation_space.shape\n",
        "state_size = env.observation_space.shape[0]\n",
        "number_actions = env.action_space.n\n",
        "print('State shape: ', state_shape)\n",
        "print('State size: ', state_size)\n",
        "print('Number of actions: ', number_actions)"
      ],
      "metadata": {
        "id": "dx8WQ7lbeY_z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b153711d-98ca-49f7-e2eb-a575e1b90cec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "State shape:  (210, 160, 3)\n",
            "State size:  210\n",
            "Number of actions:  9\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gymnasium/envs/registration.py:517: DeprecationWarning: \u001b[33mWARN: The environment MsPacmanDeterministic-v0 is out of date. You should consider upgrading to version `v4`.\u001b[0m\n",
            "  logger.deprecation(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Initializing the hyperparameters"
      ],
      "metadata": {
        "id": "Bx6IdX3ciDqH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 5e-4 # From expermintation for Training Ai to play pacman\n",
        "minibatch_size = 64 # Number of observations used in one step of the training to update the weights\n",
        "discount_factor = 0.99 # Close to one to make the agent look for accumlated future reward (not being short sighted)\n",
        "# REPLAY MEMORY NOT NEEDED FOR CONVOLUTIONAL DEPP Q - LEARNING\n",
        "# Soft update not needed for this specific enviroment"
      ],
      "metadata": {
        "id": "G5m_OpkewSq3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preprocessing the frames"
      ],
      "metadata": {
        "id": "U2bDShIEkA5V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# To make the input images converted into Pytorch tensors\n",
        "# So they can be fed into the ANN\n",
        "\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "\n",
        "def preprocess_frame(frame):\n",
        "  #convert the numpy array into PIL image\n",
        "  frame = Image.fromarray(frame)\n",
        "  # Do pre-processing (making the dimesnsions smaller and reshape it into squares 128x128 pixel)\n",
        "  preprocess = transforms.Compose([transforms.Resize((128, 128)), transforms.ToTensor()])\n",
        "  return preprocess(frame).unsqueeze(0) # .unsqueeze(0) to keep track of which batch each frame belongs to, and set it to the first dimension"
      ],
      "metadata": {
        "id": "yYNsWQ9uw9f9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Implementing the DCQN class"
      ],
      "metadata": {
        "id": "imMdSO-HAWra"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Agent():\n",
        "\n",
        "# state size not needed for images input\n",
        "\n",
        "  def __init__(self, action_size):\n",
        "    self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    self.action_size = action_size\n",
        "\n",
        "    self.local_qnetwork = Network(action_size).to(self.device)\n",
        "    self.target_qnetwork = Network(action_size).to(self.device)\n",
        "\n",
        "    self.optimizer = optim.Adam(self.local_qnetwork.parameters(), lr = learning_rate) # Intiallizing the optimizer\n",
        "    self.memory = deque(maxlen= 10000) # instead of the replay memory\n",
        "\n",
        "  # Store experiences and decide when to learn from them\n",
        "  def step(self, state, action, reward, next_state, done):\n",
        "\n",
        "    #pre-process the state and the next state\n",
        "    state = preprocess_frame(state)\n",
        "    next_state = preprocess_frame(next_state)\n",
        "\n",
        "    # append experience to the memory as tuple\n",
        "    self.memory.append((state, action, reward, next_state, done))\n",
        "\n",
        "    if len(self.memory) > minibatch_size: #there are at least 64 of observations\n",
        "      experiences = random.sample(self.memory, k = minibatch_size) # take random 64 sample (minibatch) of the observations from the memory\n",
        "\n",
        "      # then learn\n",
        "      self.learn(experiences, discount_factor)\n",
        "\n",
        "  #Act method thatt will select an action based on a given state and certain epsilon value for an epsilon greedy action selection policy\n",
        "  def act(self, state, epsilon = 0.):\n",
        "\n",
        "    state = preprocess_frame(state).to(self.device) # preprocess the state\n",
        "    self.local_qnetwork.eval # putting the local q network in evaluation mode\n",
        "\n",
        "    # do check that we are in predection (inference) mode not training mode\n",
        "    with torch.no_grad():\n",
        "      # Now we making prediction\n",
        "      action_values = self.local_qnetwork(state)\n",
        "\n",
        "    #return back to training mode\n",
        "    self.local_qnetwork.train()\n",
        "\n",
        "    # Now use the epsilon, generate random number, if the random number > epsilon, then select the action number with the highest q value, else select random action\n",
        "    if random.random() > epsilon:\n",
        "      return np.argmax(action_values.cpu().data.numpy())\n",
        "    else:\n",
        "      return random.choice(np.arange(self.action_size))\n",
        "\n",
        "  # Make the learn method\n",
        "  def learn(self, experiences, discount_factor):\n",
        "\n",
        "    # Implementing Eligibility Trace (stacking the experience elements)\n",
        "    states, actions, rewards, next_states, dones = zip(*experiences)\n",
        "\n",
        "    states = torch.from_numpy(np.vstack(states)).float().to(self.device) #stacking all the states from the sampled experienced together, # conver states into pytorch tensors, # Convert them to float, # Make sure this functions whether CPU or GPU\n",
        "    actions = torch.from_numpy(np.vstack(actions)).long().to(self.device) #stacking all the actions from the sampled experienced together, # conver states into pytorch tensors, # Convert them to long integers, # Make sure this functions whether CPU or GPU\n",
        "    rewards = torch.from_numpy(np.vstack(rewards)).float().to(self.device) #stacking all the rewards from the sampled experienced together, # conver states into pytorch tensors, # Convert them to float, # Make sure this functions whether CPU or GPU\n",
        "    next_states = torch.from_numpy(np.vstack(next_states)).float().to(self.device) #stacking all the next states from the sampled experienced together, # conver states into pytorch tensors, # Convert them to float, # Make sure this functions whether CPU or GPU\n",
        "    dones = torch.from_numpy(np.vstack(dones).astype(np.uint8)).float().to(self.device) #stacking all the dones from the sampled experienced together, # conver states into pytorch tensors, # Convert them to boolean, # Make sure this functions whether CPU or GPU\n",
        "\n",
        "\n",
        "\n",
        "    # prepare to compute Cross-Entropy Function\n",
        "    next_q_targets = self.target_qnetwork(next_states).detach().max(1)[0].unsqueeze(1) # forward propagate next state from our target q network, this gives the action values of our target q network propagating the next state, detatch the action values in the tensror, since we want to take the maximum q values, we need the maximum value along dimension 1, square bracket zero is because we dont want its indices\n",
        "    q_targets = rewards + (discount_factor * next_q_targets * (1 - dones))\n",
        "\n",
        "    q_expected = self.local_qnetwork(states).gather(1, actions)\n",
        "\n",
        "    # Compute the loss function (Cross-Entropy)\n",
        "    loss = F.mse_loss(q_expected, q_targets)\n",
        "\n",
        "    # Intialize the optimizer (reset it)\n",
        "    self.optimizer.zero_grad()\n",
        "\n",
        "    # Back Propagate the loss\n",
        "    loss.backward()\n",
        "\n",
        "    # single optimization step\n",
        "    self.optimizer.step()\n",
        "\n",
        "# Self update not needed\n",
        "\n",
        "      # # Update the target network parameters with thios of local network parameters\n",
        "      # self.soft_update(self.local_qnetwork, self.target_qnetwork, interpolation_parameter)\n",
        "\n",
        "  # # Method that will update the parameters\n",
        "  # def soft_update(self, local_model, target_model, interpolation_parameter):\n",
        "  #   for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
        "  #     target_param.data.copy_(interpolation_parameter * local_param.data + (1.0 - interpolation_parameter) * target_param.data)\n"
      ],
      "metadata": {
        "id": "VGcwh7A9zgeV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Initializing the DCQN agent"
      ],
      "metadata": {
        "id": "yUg95iBpAwII"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "agent = Agent(action_size = number_actions)"
      ],
      "metadata": {
        "id": "RFBKwwIy58tY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training the DCQN agent"
      ],
      "metadata": {
        "id": "CK6Zt_gNmHvm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "number_episodes = 2000\n",
        "max_number_timesteps_per_episode = 10000\n",
        "epsilon_starting_value = 1.0\n",
        "epsilon_ending_value = 0.01\n",
        "epsilon_decay_value = 0.995\n",
        "epsilon = epsilon_starting_value\n",
        "scores_on_100_episodes = deque(maxlen = 100)\n",
        "\n",
        "for episodes in range(1, number_episodes + 1):\n",
        "  # reset enviroment to intial state\n",
        "  state, _ = env.reset()\n",
        "\n",
        "  # intialize the score (cumulative reward)\n",
        "  score = 0\n",
        "\n",
        "  for t in range(max_number_timesteps_per_episode):\n",
        "    action = agent.act(state, epsilon)\n",
        "    next_state, reward, done, _, _ = env.step(action)\n",
        "\n",
        "    agent.step(state=state, action=action, reward=reward, next_state=next_state, done=done)\n",
        "    state = next_state\n",
        "    score += reward\n",
        "\n",
        "    if done:\n",
        "      break\n",
        "\n",
        "  scores_on_100_episodes.append(score)\n",
        "  epsilon = max(epsilon_ending_value, epsilon_decay_value * epsilon)\n",
        "\n",
        "  # Dynamic print\n",
        "  print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(episodes, np.mean(scores_on_100_episodes)), end = \"\")\n",
        "  if episodes % 100 == 0:\n",
        "    print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(episodes, np.mean(scores_on_100_episodes)))\n",
        "\n",
        "  if np.mean(scores_on_100_episodes) >= 500.0:\n",
        "    print('\\nEnviroment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(episodes - 100, np.mean(scores_on_100_episodes)))\n",
        "\n",
        "    torch.save(agent.local_qnetwork.state_dict(), 'checkpoint.pth')\n",
        "    break #exit training"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eWcnEcLS6G6f",
        "outputId": "6fe1e189-811f-4d14-df69-f2d6f5eddced"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 100\tAverage Score: 323.80\n",
            "Episode 200\tAverage Score: 364.60\n",
            "Episode 300\tAverage Score: 383.40\n",
            "Episode 400\tAverage Score: 427.40\n",
            "Episode 499\tAverage Score: 443.00"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 3 - Visualizing the results"
      ],
      "metadata": {
        "id": "-0WhhBV8nQdf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cb9nVvU2Okhk"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "import io\n",
        "import base64\n",
        "import imageio\n",
        "from IPython.display import HTML, display\n",
        "\n",
        "def show_video_of_model(agent, env_name):\n",
        "    env = gym.make(env_name, render_mode='rgb_array')\n",
        "    state, _ = env.reset()\n",
        "    done = False\n",
        "    frames = []\n",
        "    while not done:\n",
        "        frame = env.render()\n",
        "        frames.append(frame)\n",
        "        action = agent.act(state)\n",
        "        state, reward, done, _, _ = env.step(action)\n",
        "    env.close()\n",
        "    imageio.mimsave('video.mp4', frames, fps=30)\n",
        "\n",
        "show_video_of_model(agent, 'MsPacmanDeterministic-v0')\n",
        "\n",
        "def show_video():\n",
        "    mp4list = glob.glob('*.mp4')\n",
        "    if len(mp4list) > 0:\n",
        "        mp4 = mp4list[0]\n",
        "        video = io.open(mp4, 'r+b').read()\n",
        "        encoded = base64.b64encode(video)\n",
        "        display(HTML(data='''<video alt=\"test\" autoplay\n",
        "                loop controls style=\"height: 400px;\">\n",
        "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "             </video>'''.format(encoded.decode('ascii'))))\n",
        "    else:\n",
        "        print(\"Could not find video\")\n",
        "\n",
        "show_video()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}